apiVersion: apps/v1
kind: Deployment
metadata:
  name: "my-triton-inference-server-deployment"
  labels:
    app: "my-triton-inference-server"
spec:
  replicas: 1
  selector:
    matchLabels:
      app: "my-triton-inference-server"
  template:
    metadata:
      labels:
        app: "my-triton-inference-server"

    spec:
      containers:
        - name: "my-triton-inference-server"
          image: "quay.io/ibm/tritonserver-ppc64le:v2.13.0"
          imagePullPolicy: "IfNotPresent"

          resources:
            limits:
              nvidia.com/gpu: 0

          args: ["tritonserver", "--model-store=s3://minio-service-kubeflow.apps:80/my-model-repository/my-model", 
                 "--model-control-mode=poll",
                 "--repository-poll-secs=5"]

          env:
          - name: AWS_ACCESS_KEY_ID
            valueFrom:
              secretKeyRef:
                name: minio-credentials
                key: AWS_ACCESS_KEY_ID
          - name: AWS_SECRET_ACCESS_KEY
            valueFrom:
              secretKeyRef:
                name: minio-credentials
                key: AWS_SECRET_ACCESS_KEY

          ports:
            - containerPort: 8000
              name: http
            - containerPort: 8001
              name: grpc
            - containerPort: 8002
              name: metrics
          livenessProbe:
            httpGet:
              path: /v2/health/live
              port: http
          readinessProbe:
            initialDelaySeconds: 5
            periodSeconds: 5
            httpGet:
              path: /v2/health/ready
              port: http

              #      securityContext:
              #        runAsUser: 1000
              #        fsGroup: 1000


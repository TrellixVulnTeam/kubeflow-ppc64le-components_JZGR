{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f20b995-81e6-46ae-952a-f36267ee6cea",
   "metadata": {},
   "source": [
    "# Train Model Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "266a528d-a77c-460d-8b4c-3632170c702c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp.components import (\n",
    "    create_component_from_func,\n",
    "    InputPath,\n",
    "    OutputPath\n",
    ")\n",
    "from typing import Dict\n",
    "\n",
    "BASE_IMAGE = \"quay.io/ibm/kubeflow-notebook-image-ppc64le:latest\"\n",
    "\n",
    "\n",
    "def train_model_job(\n",
    "        dataset_directory: InputPath(str),\n",
    "        train_specification: str,\n",
    "        train_parameters: Dict[str, str],\n",
    "        model_dir: OutputPath(str),\n",
    "        train_mount: str = \"/train\",\n",
    "        model_name: str = \"my-model\",\n",
    "        base_image: str = \"quay.io/ibm/kubeflow-notebook-image-ppc64le:latest\",\n",
    "        namespace: str = \"\",\n",
    "        node_selector: str = \"\",\n",
    "        pvc_name: str = \"\",\n",
    "        pvc_size: str = \"10Gi\",\n",
    "        cpus: str = \"\",\n",
    "        gpus: int = 0,\n",
    "        memory: str = \"\",\n",
    "        remote_host: str = \"\",\n",
    "        minio_url: str = \"minio-service.kubeflow:9000\",\n",
    "        minio_secret: str = \"mlpipeline-minio-artifact\",\n",
    "        upload_bucket: str = \"jobdata\"\n",
    "):\n",
    "    '''\n",
    "    Trains a model. Once trained, the model is persisted to model_dir.\n",
    "\n",
    "            Parameters:\n",
    "                    dataset_directory: Path to the directory with training data.\n",
    "                    train_specification: Training command as generated from a Python function using kfp.components.func_to_component_text.\n",
    "                    train_parameters: Dictionary mapping formal to actual parameters for the training spacification.\n",
    "                    model_dir: Target path where the model will be stored.\n",
    "                    train_mount: Optional mounting point for training data of an existing PVC. Example: \"/train\".\n",
    "                    model_name: Optional name of the model. Must be unique for the targeted namespace and conform Kubernetes naming conventions. Example: my-model.\n",
    "                    base_image: Optional base image for model training. Example: quay.io/ibm/kubeflow-notebook-image-ppc64le:latest.\n",
    "                    namespace: Optional namespace where the Job and associated volumes will be created. By default, the same namespace as where the pipeline is executed is chosen. Example: \"user-example-com\".\n",
    "                    node_selector: Optional node selector for worker nodes. Example: nvidia.com/gpu.product: \"Tesla-V100-SXM2-32GB\".\n",
    "                    pvc_name: Optional name to an existing persistent volume claim (pvc). If given, this pvc is mounted into the training job. Example: \"music-genre-classification-j4ssf-training-pvc\".\n",
    "                    pvc_size: Optional size of the storage during model training. Storage is mounted into to the Job based on a persitent volume claim of the given size. Example: 10Gi.\n",
    "                    cpus: Optional CPU limit for the job. Leave empty for cluster defaults (typically no limit). Example: \"1000m\".\n",
    "                    gpus: Optional number of GPUs for the job. Example: 2.\n",
    "                    memory: Optional memory limit for the job. Leave empty for cluster defaults (typically no limit). Example: \"1Gi\".\n",
    "                    remote_host: Optional remote kubernetes cluster to run the job in. Requires to have a suitable bearer token mounted from a secret to this component such that the environment variable TOKEN can be used. Example: \"https://XXX.XXX.XXX.XXX:443\".\n",
    "                    minio_url: Optional URL to MinIO object store. Username (MINIO_USER) and password (MINIO_PASS) have to be mounted via k8s_secret_key_to_env. Example: \"minio-service.kubeflow:9000\".\n",
    "                    upload_bucket: Optional bucket name when using MinIO for data synchronization. Example: \"temp\".\n",
    "    '''\n",
    "    from datetime import datetime\n",
    "    import errno\n",
    "    import json\n",
    "    import kfp\n",
    "    from kubernetes import (\n",
    "        client,\n",
    "        config,\n",
    "        utils,\n",
    "        watch\n",
    "    )\n",
    "    import logging\n",
    "    import os\n",
    "    import shutil\n",
    "    import sys\n",
    "    import yaml\n",
    "\n",
    "    logging.basicConfig(\n",
    "        stream=sys.stdout,\n",
    "        level=logging.INFO,\n",
    "        format='%(levelname)s %(asctime)s: %(message)s'\n",
    "    )\n",
    "    logger = logging.getLogger()\n",
    "\n",
    "    ###########################################################################\n",
    "    # Helper Functions\n",
    "    ###########################################################################\n",
    "    def establish_kubernetes_connection():\n",
    "        token = os.getenv('TOKEN')\n",
    "        if (token == \"\" or remote_host == \"\"):\n",
    "            logger.info(\"Token and remote_host not set. Using in-cluster configuration...\")\n",
    "            config.load_incluster_config()\n",
    "            api_client = client.ApiClient()\n",
    "            is_remote = False\n",
    "            if(not os.path.exists(train_mount)):\n",
    "                logger.warning(f\"No local mount to {train_mount} found. Therefore, switching to remote data synchronization mode via MinIO. This will work but is slower compared to local mounts. Consider adding a mount to '{train_mount}' for this component by using a PVC inside your pipeline.\")\n",
    "                is_remote = True\n",
    "        else:\n",
    "            # see: https://github.com/kubernetes-client/python/blob/6d4587e18064288d031ed9bbf5ab5b8245460b3c/examples/remote_cluster.py\n",
    "            logger.info(\"Token and remote_host found. Using remote cluster configuration...\")\n",
    "            configuration = client.Configuration()\n",
    "            configuration.host = remote_host\n",
    "            configuration.verify_ssl = False\n",
    "            configuration.api_key = {\"authorization\": \"Bearer \" + token}\n",
    "            api_client = client.ApiClient(configuration)\n",
    "            is_remote = True\n",
    "\n",
    "        return (api_client, is_remote)\n",
    "\n",
    "    def clone_path(source, target):\n",
    "        try:\n",
    "            logger.info(f\"Cloning source path {source} to {target} of training job...\")\n",
    "            shutil.copytree(source, target)\n",
    "            logger.info(\"Cloning finished. Target path contents:\")\n",
    "            logger.info(os.listdir(target))\n",
    "        except OSError as e:\n",
    "            if e.errno in (errno.ENOTDIR, errno.EINVAL):\n",
    "                shutil.copy(source, target)\n",
    "            else:\n",
    "                raise\n",
    "\n",
    "    def sync_with_minio(\n",
    "        inputs: dict,\n",
    "        job_name: str,\n",
    "        upload_bucket: str,\n",
    "        minio_url: str,\n",
    "        is_upload: bool,\n",
    "        remove_minio_files: bool = False\n",
    "    ):\n",
    "        import json\n",
    "        import logging\n",
    "        from minio import Minio\n",
    "        import os\n",
    "        import sys\n",
    "        import tarfile\n",
    "\n",
    "        logging.basicConfig(\n",
    "            stream=sys.stdout,\n",
    "            level=logging.INFO,\n",
    "            format='%(levelname)s %(asctime)s: %(message)s'\n",
    "        )\n",
    "        logger = logging.getLogger()\n",
    "\n",
    "        def establish_minio_connection(minio_url):\n",
    "            minio_user = os.getenv('MINIO_USER')\n",
    "            minio_pass = os.getenv('MINIO_PASS')\n",
    "\n",
    "            if (minio_user == \"\" or minio_pass == \"\"):\n",
    "                err = \"Environment variables MINIO_USER and MINIO_PASS need externally to be provided to this component using k8s_secret_key_to_env!\"\n",
    "                logger.error(err)\n",
    "                raise Exception(err)\n",
    "\n",
    "            return Minio(\n",
    "                minio_url,\n",
    "                access_key=minio_user,\n",
    "                secret_key=minio_pass,\n",
    "                secure=False\n",
    "            )\n",
    "\n",
    "        def path_to_tarfilename(pathname):\n",
    "            return f\"{pathname.replace(os.sep, '-')}.tar.gz\"\n",
    "\n",
    "        def make_tarfile(output_filename, source_dir):\n",
    "            with tarfile.open(output_filename, \"w:gz\") as tar:\n",
    "                tar.add(source_dir, arcname='.')\n",
    "\n",
    "        def upload_to_minio(file, upload_bucket, job_name, minio_client):\n",
    "            # Create models bucket if it does not yet exist\n",
    "            response = minio_client.list_buckets()\n",
    "            models_bucket_exists = False\n",
    "            for bucket in response:\n",
    "                if bucket.name == upload_bucket:\n",
    "                    models_bucket_exists = True\n",
    "\n",
    "            if not models_bucket_exists:\n",
    "                minio_client.make_bucket(bucket_name=upload_bucket)\n",
    "\n",
    "            minio_client.fput_object(\n",
    "                bucket_name=upload_bucket,  # bucket name in Minio\n",
    "                object_name=f\"{job_name}/{file}\",  # file name in bucket of Minio\n",
    "                file_path=file,  # file path / name in local system\n",
    "            )\n",
    "\n",
    "        def download_from_minio(file, upload_bucket, job_name, minio_client, remove_minio_file):\n",
    "            obj = f\"{job_name}/{file}\"\n",
    "            data = minio_client.get_object(upload_bucket, obj)\n",
    "            with open(file, 'wb') as file_data:\n",
    "                for d in data.stream(32*1024):\n",
    "                    file_data.write(d)\n",
    "            if remove_minio_file:\n",
    "                minio_client.remove_object(upload_bucket, obj)\n",
    "\n",
    "        def extract_tarfile(tarfile_name, target):\n",
    "            with tarfile.open(tarfile_name, \"r:gz\") as tar_gz_ref:\n",
    "                tar_gz_ref.extractall(target)\n",
    "\n",
    "        logger.info(\"Establishing MinIO connection...\")\n",
    "        minio_client = establish_minio_connection(minio_url)\n",
    "\n",
    "        if (isinstance(inputs, str)):\n",
    "            inputs = json.loads(inputs)\n",
    "\n",
    "        if (isinstance(is_upload, str)):\n",
    "            if (is_upload == \"True\"):\n",
    "                is_upload = True\n",
    "            else:\n",
    "                is_upload = False\n",
    "\n",
    "        for (source, target) in inputs:\n",
    "            tarfilename = path_to_tarfilename(source)\n",
    "\n",
    "            if (is_upload):\n",
    "                logger.info(f\"Tar.gz input {source} into {tarfilename}...\")\n",
    "                make_tarfile(tarfilename, source)\n",
    "\n",
    "                logger.info(f'Uploading {tarfilename} to {upload_bucket}/{job_name}/{tarfilename}...')\n",
    "                upload_to_minio(tarfilename, upload_bucket, job_name, minio_client)\n",
    "            else:\n",
    "                logger.info(f'Downloading {upload_bucket}/{job_name}/{tarfilename} to {tarfilename}...')\n",
    "                download_from_minio(tarfilename, upload_bucket, job_name, minio_client, remove_minio_files)\n",
    "\n",
    "                logger.info(f'Extracting {tarfilename} to {target}...')\n",
    "                extract_tarfile(tarfilename, target)\n",
    "\n",
    "                logger.info('Result:')\n",
    "                logger.info(os.listdir(target))\n",
    "\n",
    "    def generate_unique_job_name(model_name: str):\n",
    "        epoch = datetime.today().strftime('%Y%m%d%H%M%S')\n",
    "        return f\"job-{model_name}-{epoch}\"\n",
    "\n",
    "    def initialize_namespace(namespace: str):\n",
    "        SA_NAMESPACE = \"/var/run/secrets/kubernetes.io/serviceaccount/namespace\"\n",
    "\n",
    "        if namespace == \"\":\n",
    "            with open(SA_NAMESPACE) as f:\n",
    "                namespace = f.read()\n",
    "        namespace_spec = f\"namespace: {namespace}\"\n",
    "\n",
    "        return (namespace, namespace_spec)\n",
    "\n",
    "    def initialize_nodeselector(node_selector: str):\n",
    "        if node_selector != \"\":\n",
    "            node_selector = f\"nodeSelector:\\n        {node_selector}\"\n",
    "        return node_selector\n",
    "\n",
    "    def initialize_init_container(\n",
    "        is_remote: bool,\n",
    "        base_image: str,\n",
    "        inputs: Dict[str, str],\n",
    "        job_name: str,\n",
    "        upload_bucket: str,\n",
    "        minio_url: str,\n",
    "        minio_secret: str,\n",
    "        mount_path: str\n",
    "    ):\n",
    "        if(not is_remote):\n",
    "            return \"\"\n",
    "\n",
    "        command_specification = kfp.components.func_to_component_text(\n",
    "            func=sync_with_minio\n",
    "        )\n",
    "\n",
    "        # inner components loose type information as needed by lists/dicts\n",
    "        # -> inputs need to be a string (using json here)\n",
    "        inputs_json = json.dumps(inputs)\n",
    "        parameters = {\n",
    "            \"inputs\": inputs_json,\n",
    "            \"job_name\": job_name,\n",
    "            \"upload_bucket\": upload_bucket,\n",
    "            \"minio_url\": minio_url,\n",
    "            \"is_upload\": \"False\"\n",
    "        }\n",
    "\n",
    "        command, _, _ = initialize_command(\n",
    "            command_specification,\n",
    "            parameters\n",
    "        )\n",
    "\n",
    "        init_container = f\"\"\"initContainers:\n",
    "          - name: init-inputs\n",
    "            image: {base_image}\n",
    "            command: {command}\n",
    "            volumeMounts:\n",
    "            - mountPath: {mount_path}\n",
    "              name: training\n",
    "            env:\n",
    "            - name: MINIO_USER\n",
    "              valueFrom:\n",
    "                secretKeyRef:\n",
    "                  name: {minio_secret}\n",
    "                  key: accesskey\n",
    "                  optional: false\n",
    "            - name: MINIO_PASS\n",
    "              valueFrom:\n",
    "                secretKeyRef:\n",
    "                  name: {minio_secret}\n",
    "                  key: secretkey\n",
    "                  optional: false\n",
    "\"\"\"\n",
    "        return init_container\n",
    "\n",
    "    def initialize_command(\n",
    "        specification: str,\n",
    "        parameters: Dict[str, str],\n",
    "        path_parameters: Dict[str, str] = {},\n",
    "        mount_path: str = \"/tmp\"\n",
    "    ):\n",
    "        component_yaml = yaml.safe_load(specification)\n",
    "        container_yaml = component_yaml[\"implementation\"][\"container\"]\n",
    "        command = container_yaml[\"command\"]\n",
    "        args = container_yaml[\"args\"]\n",
    "\n",
    "        actual_args = list()\n",
    "        inputs = list()\n",
    "        outputs = list()\n",
    "        for idx, arg in enumerate(args):\n",
    "            if type(arg) is dict:\n",
    "                if \"inputValue\" in arg:\n",
    "                    # required parameter (value)\n",
    "                    key = arg[\"inputValue\"]\n",
    "                    if key in parameters:\n",
    "                        actual_args.append(parameters[key])\n",
    "                    else:\n",
    "                        err = f\"Required parameter '{key}' missing in component input!\"\n",
    "                        logger.error(err)\n",
    "                        raise Exception(err)\n",
    "                elif \"if\" in arg:\n",
    "                    # optional parameter\n",
    "                    key = arg[\"if\"][\"cond\"][\"isPresent\"]\n",
    "                    if key in parameters:\n",
    "                        actual_args.append(f\"--{key}\")\n",
    "                        actual_args.append(parameters[key])\n",
    "                elif \"inputPath\" in arg:\n",
    "                    # required InputPath\n",
    "                    key = arg[\"inputPath\"]\n",
    "                    if key in parameters:\n",
    "                        path_key = parameters[key]\n",
    "                        if path_key in path_parameters:\n",
    "                            mount = f\"{mount_path}{path_parameters[path_key]}\"\n",
    "                            inputs.append((path_parameters[path_key], mount))\n",
    "                            actual_args.append(mount)\n",
    "                        else:\n",
    "                            err = f\"InputPath '{path_key}' unavailable in training component!\"\n",
    "                            logger.error(err)\n",
    "                            raise Exception(err)\n",
    "                    else:\n",
    "                        err = f\"Required parameter '{key}' missing in component input!\"\n",
    "                        logger.error(err)\n",
    "                        raise Exception(err)\n",
    "                elif \"outputPath\" in arg:\n",
    "                    # required OutputPath\n",
    "                    key = arg[\"outputPath\"]\n",
    "                    if key in parameters:\n",
    "                        path_key = parameters[key]\n",
    "                        if path_key in path_parameters:\n",
    "                            mount = f\"{mount_path}{path_parameters[path_key]}\"\n",
    "                            outputs.append((mount, path_parameters[path_key]))\n",
    "                            actual_args.append(mount)\n",
    "                        else:\n",
    "                            err = f\"OutputPath '{path_key}' unavailable in training component!\"\n",
    "                            logger.error(err)\n",
    "                            raise Exception(err)\n",
    "                    else:\n",
    "                        err = f\"Required parameter '{key}' missing in component input!\"\n",
    "                        logger.error(err)\n",
    "                        raise Exception(err)\n",
    "            else:\n",
    "                # required parameter (key)\n",
    "                actual_args.append(arg)\n",
    "\n",
    "        command_with_initialized_args = json.dumps(command + actual_args)\n",
    "\n",
    "        return command_with_initialized_args, inputs, outputs\n",
    "\n",
    "    def initialize_fetch_command(\n",
    "        outputs: Dict[str, str],\n",
    "        job_name: str,\n",
    "        upload_bucket: str,\n",
    "        minio_url: str,\n",
    "    ):\n",
    "        command_specification = kfp.components.func_to_component_text(\n",
    "            func=sync_with_minio\n",
    "        )\n",
    "\n",
    "        # inner components loose type information as needed by lists/dicts\n",
    "        # -> outputs need to be a string (using json here)\n",
    "        outputs_json = json.dumps(outputs)\n",
    "        parameters = {\n",
    "            \"inputs\": outputs_json,\n",
    "            \"job_name\": job_name,\n",
    "            \"upload_bucket\": upload_bucket,\n",
    "            \"minio_url\": minio_url,\n",
    "            \"is_upload\": \"True\"\n",
    "        }\n",
    "        command, _, _ = initialize_command(\n",
    "            command_specification,\n",
    "            parameters\n",
    "        )\n",
    "        return command\n",
    "\n",
    "    def create_pvc_spec(pvc_name, namespace_spec, pvc_size):\n",
    "        pvc_spec = f\"\"\"apiVersion: batch/v1\n",
    "apiVersion: v1\n",
    "kind: PersistentVolumeClaim\n",
    "metadata:\n",
    "  name: {pvc_name}\n",
    "  {namespace_spec}\n",
    "spec:\n",
    "  accessModes:\n",
    "  - ReadWriteMany\n",
    "  resources:\n",
    "    requests:\n",
    "      storage: {pvc_size}\n",
    "\"\"\"\n",
    "        return yaml.safe_load(pvc_spec)\n",
    "\n",
    "    def create_minio_secret_spec(namespace_spec, minio_secret):\n",
    "        minio_user = os.getenv('MINIO_USER')\n",
    "        minio_pass = os.getenv('MINIO_PASS')\n",
    "        minio_secret_spec = f\"\"\"apiVersion: v1\n",
    "kind: Secret\n",
    "metadata:\n",
    "  name: {minio_secret}\n",
    "  {namespace_spec}\n",
    "stringData:\n",
    "  accesskey: {minio_user}\n",
    "  secretkey: {minio_pass}\n",
    "\"\"\"\n",
    "        return yaml.safe_load(minio_secret_spec)\n",
    "\n",
    "    def create_train_job_spec(\n",
    "        job_name,\n",
    "        namespace_spec,\n",
    "        node_selector,\n",
    "        base_image,\n",
    "        train_command,\n",
    "        train_mount,\n",
    "        cpus,\n",
    "        memory,\n",
    "        gpus,\n",
    "        init_container,\n",
    "        pvc_name\n",
    "    ):\n",
    "        if cpus:\n",
    "            cpus = f\"cpu: {cpus}\"\n",
    "        if memory:\n",
    "            memory = f\"memory: {memory}\"\n",
    "        if gpus:\n",
    "            gpus = f\"nvidia.com/gpu: {gpus}\"\n",
    "\n",
    "        job_spec = f\"\"\"apiVersion: batch/v1\n",
    "kind: Job\n",
    "metadata:\n",
    "  name: {job_name}\n",
    "  {namespace_spec}\n",
    "spec:\n",
    "  template:\n",
    "    metadata:\n",
    "      annotations:\n",
    "        sidecar.istio.io/inject: \"false\"\n",
    "    spec:\n",
    "      {node_selector}\n",
    "      containers:\n",
    "        - name: training-container\n",
    "          image: {base_image}\n",
    "          command: {train_command}\n",
    "          volumeMounts:\n",
    "            - mountPath: {train_mount}\n",
    "              name: training\n",
    "          restartPolicy: Never\n",
    "          resources:\n",
    "            limits:\n",
    "              {cpus}\n",
    "              {memory}\n",
    "              {gpus}\n",
    "      {init_container}\n",
    "      volumes:\n",
    "        - name: training\n",
    "          persistentVolumeClaim:\n",
    "            claimName: {pvc_name}\n",
    "      restartPolicy: Never\n",
    "\"\"\"\n",
    "        return yaml.safe_load(job_spec)\n",
    "\n",
    "    def create_fetch_job_spec(\n",
    "        job_name,\n",
    "        namespace_spec,\n",
    "        base_image,\n",
    "        fetch_command,\n",
    "        train_mount,\n",
    "        minio_secret,\n",
    "        pvc_name\n",
    "    ):\n",
    "        job_spec = f\"\"\"apiVersion: batch/v1\n",
    "kind: Job\n",
    "metadata:\n",
    "  name: {job_name}\n",
    "  {namespace_spec}\n",
    "spec:\n",
    "  template:\n",
    "    metadata:\n",
    "      annotations:\n",
    "        sidecar.istio.io/inject: \"false\"\n",
    "    spec:\n",
    "      containers:\n",
    "        - name: training-container\n",
    "          image: {base_image}\n",
    "          command: {fetch_command}\n",
    "          volumeMounts:\n",
    "            - mountPath: {train_mount}\n",
    "              name: training\n",
    "          restartPolicy: Never\n",
    "          env:\n",
    "          - name: MINIO_USER\n",
    "            valueFrom:\n",
    "              secretKeyRef:\n",
    "                name: {minio_secret}\n",
    "                key: accesskey\n",
    "                optional: false\n",
    "          - name: MINIO_PASS\n",
    "            valueFrom:\n",
    "              secretKeyRef:\n",
    "                name: {minio_secret}\n",
    "                key: secretkey\n",
    "                optional: false\n",
    "      volumes:\n",
    "        - name: training\n",
    "          persistentVolumeClaim:\n",
    "            claimName: {pvc_name}\n",
    "      restartPolicy: Never\n",
    "\"\"\"\n",
    "        return yaml.safe_load(job_spec)\n",
    "\n",
    "    def submit_and_monitor_job(job_spec, namespace, api_client):\n",
    "        objects = utils.create_from_yaml(\n",
    "            api_client,\n",
    "            yaml_objects=[job_spec]\n",
    "        )\n",
    "        job_name = objects[0][0].metadata.name\n",
    "\n",
    "        logger.info(\"Waiting for job to succeed...\")\n",
    "        job_watch = watch.Watch()\n",
    "        for job_event in job_watch.stream(\n",
    "            batch_api.list_namespaced_job,\n",
    "            namespace=namespace,\n",
    "            label_selector=f\"job-name={job_name}\",\n",
    "            timeout_seconds=0\n",
    "        ):\n",
    "            job = job_event['object']\n",
    "            if job.status.active and not job.status.failed:\n",
    "                logger.info(\"Monitoring pods of job...\")\n",
    "\n",
    "                # See https://stackoverflow.com/questions/65938572/kubernetes-python-client-equivalent-of-kubectl-wait-for-command\n",
    "                pod_watch = watch.Watch()\n",
    "                for pod_event in pod_watch.stream(func=core_api.list_namespaced_pod,\n",
    "                                          namespace=namespace,\n",
    "                                          label_selector=\"controller-uid=\" + job.metadata.labels[\"controller-uid\"],\n",
    "                                          timeout_seconds=0):\n",
    "                    pod = pod_event[\"object\"]\n",
    "                    pod_name = pod.metadata.name\n",
    "\n",
    "                    logger.info(f\"Pod {pod_name} status: {pod.status.phase}\")\n",
    "                    if pod.status.phase == \"Running\" or pod.status.phase == \"Succeeded\" or pod.status.phase == \"Failed\":\n",
    "                        logger.info(\"==============================================================================\")\n",
    "                        logger.info(\"==============================================================================\")\n",
    "                        logger.info(f\"=== Streaming logs of pod {pod_name}...\")\n",
    "                        logger.info(\"==============================================================================\")\n",
    "                        logger.info(\"==============================================================================\")\n",
    "\n",
    "                        log_watch = watch.Watch()\n",
    "                        for log_event in log_watch.stream(\n",
    "                            core_api.read_namespaced_pod_log,\n",
    "                            name=pod_name,\n",
    "                            namespace=namespace,\n",
    "                            follow=True,\n",
    "                            _return_http_data_only=True,\n",
    "                            _preload_content=False\n",
    "                        ):\n",
    "                            print(log_event)\n",
    "                        logger.info(\"==============================================================================\")\n",
    "                        logger.info(\"==============================================================================\")\n",
    "\n",
    "                        pod_watch.stop()\n",
    "\n",
    "                        if pod.status.phase == \"Failed\":\n",
    "                            err = \"Job failed while executing.\"\n",
    "                            logger.error(err)\n",
    "                            raise Exception(err)\n",
    "                        break\n",
    "                    if pod_event[\"type\"] == \"DELETED\":\n",
    "                        err = \"Pod was deleted while we where waiting for it to start.\"\n",
    "                        logger.error(err)\n",
    "                        raise Exception(err)\n",
    "            if job.status.succeeded:\n",
    "                job_watch.stop()\n",
    "                logger.info(\"Job finished.\")\n",
    "                break\n",
    "\n",
    "            if not job.status.active and job.status.failed:\n",
    "                job_watch.stop()\n",
    "                logger.error(\"Job failed!\")\n",
    "                raise Exception(\"Job failed!\")\n",
    "\n",
    "    ###########################################################################\n",
    "    # Main Workflow\n",
    "    ###########################################################################\n",
    "\n",
    "    logger.info(\"Establishing cluster connection...\")\n",
    "    api_client, is_remote = establish_kubernetes_connection()\n",
    "    batch_api = client.BatchV1Api(api_client)\n",
    "    core_api = client.CoreV1Api(api_client)\n",
    "\n",
    "    logger.info(\"Initializing configurations...\")\n",
    "    job_name = generate_unique_job_name(model_name)\n",
    "    if is_remote:\n",
    "        minio_secret = f\"{job_name}-minio-secret\"\n",
    "    namespace, namespace_spec = initialize_namespace(namespace)\n",
    "    pvc_name = f\"{job_name}-pvc\"\n",
    "    node_selector = initialize_nodeselector(node_selector)\n",
    "\n",
    "    path_parameters = {\n",
    "            \"dataset_directory\": dataset_directory,\n",
    "            \"model_dir\": model_dir\n",
    "        }\n",
    "    train_command, inputs, outputs = initialize_command(\n",
    "        train_specification,\n",
    "        train_parameters,\n",
    "        path_parameters,\n",
    "        train_mount\n",
    "    )\n",
    "    init_container = initialize_init_container(\n",
    "        is_remote,\n",
    "        base_image,\n",
    "        inputs,\n",
    "        job_name,\n",
    "        upload_bucket,\n",
    "        minio_url,\n",
    "        minio_secret,\n",
    "        train_mount\n",
    "    )\n",
    "\n",
    "    logger.info(\"=======================================\")\n",
    "    logger.info(\"Derived configurations\")\n",
    "    logger.info(\"=======================================\")\n",
    "    logger.info(f\"job_name: {job_name}\")\n",
    "    logger.info(f\"namespace: {namespace}\")\n",
    "    logger.info(f\"is_remote: {is_remote}\")\n",
    "    logger.info(f\"minio_url: {minio_url}\")\n",
    "    logger.info(f\"minio_secret: {minio_secret}\")\n",
    "    logger.info(\"inputs (input paths send to job):\")\n",
    "    for source, target in inputs:\n",
    "        logger.info(f\"- {source} -> {upload_bucket}/{job_name}/{target}\")\n",
    "    logger.info(\"outputs (output paths returning from job):\")\n",
    "    for source, target in outputs:\n",
    "        logger.info(f\"- {target} <- {upload_bucket}/{job_name}/{source}\")\n",
    "    logger.info(f\"train_command: {train_command}\")\n",
    "    logger.info(\"=======================================\")\n",
    "\n",
    "    if (is_remote):\n",
    "        logger.info(\"Using MinIO to sync data with a new remote PVC for the job...\")\n",
    "        sync_with_minio(inputs, job_name, upload_bucket, minio_url, is_upload=True)\n",
    "        pvc_spec = create_pvc_spec(pvc_name, namespace_spec, pvc_size)\n",
    "        minio_secrets_spec = create_minio_secret_spec(namespace_spec, minio_secret)\n",
    "        utils.create_from_yaml(\n",
    "            api_client,\n",
    "            yaml_objects=[pvc_spec, minio_secrets_spec]\n",
    "        )\n",
    "    else:\n",
    "        logger.info(f\"Pushing inputs to local {train_mount} mount as shared with job environment...\")\n",
    "        for (source, target) in inputs:\n",
    "            clone_path(source, target)\n",
    "\n",
    "    logger.info(\"Creating train job specification...\")\n",
    "    train_job_spec = create_train_job_spec(\n",
    "        job_name,\n",
    "        namespace_spec,\n",
    "        node_selector,\n",
    "        base_image,\n",
    "        train_command,\n",
    "        train_mount,\n",
    "        cpus,\n",
    "        memory,\n",
    "        gpus,\n",
    "        init_container,\n",
    "        pvc_name\n",
    "    )\n",
    "\n",
    "    logger.info(f\"Starting train job '{namespace}.{job_name}'...\")\n",
    "    submit_and_monitor_job(train_job_spec, namespace, api_client)\n",
    "\n",
    "    logger.info(\"Receiving training outputs...\")\n",
    "    if not os.path.exists(model_dir):\n",
    "        os.makedirs(model_dir)\n",
    "    if (is_remote):\n",
    "        fetch_command = initialize_fetch_command(\n",
    "            outputs,\n",
    "            job_name,\n",
    "            upload_bucket,\n",
    "            minio_url\n",
    "        )\n",
    "        fetch_job_name = f\"{job_name}-fetch\"\n",
    "\n",
    "        logger.info(\"Creating fetch job specification...\")\n",
    "        fetch_job_spec = create_fetch_job_spec(\n",
    "            fetch_job_name,\n",
    "            namespace_spec,\n",
    "            base_image,\n",
    "            fetch_command,\n",
    "            train_mount,\n",
    "            minio_secret,\n",
    "            pvc_name\n",
    "        )\n",
    "\n",
    "        logger.info(f\"Starting fetch job '{namespace}.{fetch_job_name}'...\")\n",
    "        submit_and_monitor_job(fetch_job_spec, namespace, api_client)\n",
    "\n",
    "        logger.info(\"Fetching output data from MinIO & deleting it afterwards...\")\n",
    "        sync_with_minio(outputs, job_name, upload_bucket, minio_url, is_upload=False, remove_minio_files=True)\n",
    "\n",
    "        logger.info(f\"Deleting Job {fetch_job_name}...\")\n",
    "        batch_api.delete_namespaced_job(fetch_job_name, namespace)\n",
    "    else:\n",
    "        logger.info(f\"Fetching outputs to local {train_mount} mount as shared with job environment...\")\n",
    "        for (source, target) in outputs:\n",
    "            clone_path(source, target)\n",
    "\n",
    "    logger.info(f\"Deleting Job {job_name}...\")\n",
    "    batch_api.delete_namespaced_job(job_name, namespace)\n",
    "    if (is_remote):\n",
    "        logger.info(f\"Deleting remote PVC {pvc_name}...\")\n",
    "        core_api.delete_namespaced_persistent_volume_claim(pvc_name, namespace)\n",
    "\n",
    "        logger.info(f\"Deleting remote MinIO secret {minio_secret}...\")\n",
    "        core_api.delete_namespaced_secret(minio_secret, namespace)\n",
    "\n",
    "    logger.info(\"Finished.\")\n",
    "\n",
    "\n",
    "train_model_job_comp = create_component_from_func(\n",
    "    func=train_model_job,\n",
    "    output_component_file='component.yaml',\n",
    "    base_image=BASE_IMAGE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322cb66a-9c1f-4efe-82df-ca7b0dfe50c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

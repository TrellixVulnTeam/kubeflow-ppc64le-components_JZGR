{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f20b995-81e6-46ae-952a-f36267ee6cea",
   "metadata": {},
   "source": [
    "# Train Model Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "266a528d-a77c-460d-8b4c-3632170c702c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp.components import (\n",
    "    create_component_from_func,\n",
    "    InputPath,\n",
    "    OutputPath\n",
    ")\n",
    "from typing import (\n",
    "    Dict,\n",
    "    NamedTuple\n",
    ")\n",
    "\n",
    "BASE_IMAGE = \"quay.io/ibm/kubeflow-notebook-image-ppc64le:latest\"\n",
    "\n",
    "\n",
    "def train_model_job(\n",
    "        dataset_directory: InputPath(str),\n",
    "        train_specification: str,\n",
    "        train_parameters: Dict[str, str],\n",
    "        model_dir: OutputPath(str),\n",
    "        train_mount: str = \"/train\",\n",
    "        model_name: str = \"my-model\",\n",
    "        base_image: str = \"quay.io/ibm/kubeflow-notebook-image-ppc64le:latest\",\n",
    "        namespace: str = \"\",\n",
    "        node_selector: str = \"\",\n",
    "        remote_host: str = \"\",\n",
    "        pvc_name: str = \"\",\n",
    "        pvc_size: str = \"10Gi\",\n",
    "        cpus: str = \"8\",\n",
    "        gpus: int = 0,\n",
    "        memory: str = \"32Gi\",\n",
    ") -> NamedTuple(\n",
    "        'TrainModelJobOutputs', [\n",
    "            ('logs', str),\n",
    "        ]):\n",
    "    '''\n",
    "    Trains a model. Once trained, the model is persisted to model_dir.\n",
    "\n",
    "            Parameters:\n",
    "                    dataset_directory: Path to the directory with training data.\n",
    "                    train_specification: Training command as generated from a Python function using kfp.components.func_to_component_text.\n",
    "                    train_parameters: Dictionary mapping formal to actual parameters for the training spacification.                    \n",
    "                    model_dir: Target path where the model will be stored.\n",
    "                    train_mount: Optional mounting point for training data of an existing PVC. Example: \"/train\".\n",
    "                    model_name: Optional name of the model. Must be unique for the targeted namespace and conform Kubernetes naming conventions. Example: my-model.\n",
    "                    base_image: Optional base image for model training. Example: quay.io/ibm/kubeflow-notebook-image-ppc64le:latest.\n",
    "                    namespace: Optional namespace where the Job and associated volumes will be created. By default, the same namespace as where the pipeline is executed is chosen. Example: \"user-example-com\".\n",
    "                    node_selector: Optional node selector for worker nodes. Example: nvidia.com/gpu.product: \"Tesla-V100-SXM2-32GB\".\n",
    "                    pvc_name: Optional name to an existing persistent volume claim (pvc). If given, this pvs is mounted into the training job. Example: \"music-genre-classification-j4ssf-training-pvc\".\n",
    "                    pvc_size: Optional size of the storage during model training. Storage is mounted into to the Job based on a persitent volume claim of the given size. Example: 10Gi.\n",
    "                    cpus: Optional CPU limit for the job. Example: \"1000m\".\n",
    "                    gpus: Optional number of GPUs for the job. Example: 2.\n",
    "                    memory: Optional memory limit for the job. Example: \"1Gi\".\n",
    "            Returns:\n",
    "                    logs: Result outputs of the Job. Example: \"...Job finished successfully\".\n",
    "    '''\n",
    "    from collections import namedtuple\n",
    "    from datetime import datetime\n",
    "    import errno\n",
    "    import json\n",
    "    from kubernetes import (\n",
    "        client,\n",
    "        config,\n",
    "        utils,\n",
    "        watch\n",
    "    )\n",
    "    from kubernetes.client.rest import ApiException\n",
    "    import logging\n",
    "    import os\n",
    "    import shutil\n",
    "    import sys\n",
    "    import yaml\n",
    "\n",
    "    logging.basicConfig(\n",
    "        stream=sys.stdout,\n",
    "        level=logging.INFO,\n",
    "        format='%(levelname)s %(asctime)s: %(message)s'\n",
    "    )\n",
    "    logger = logging.getLogger()\n",
    "\n",
    "    SA_NAMESPACE = \"/var/run/secrets/kubernetes.io/serviceaccount/namespace\"\n",
    "\n",
    "    logger.info(\"Establishing cluster connection...\")\n",
    "    config.load_incluster_config()\n",
    "\n",
    "    # init configuration variables\n",
    "    epoch = datetime.today().strftime('%Y%m%d%H%M%S')\n",
    "    job_name = f\"job-{model_name}-{epoch}\"\n",
    "\n",
    "    if namespace == \"\":\n",
    "        with open(SA_NAMESPACE) as f:\n",
    "            namespace = f.read()\n",
    "    namespace_spec = f\"namespace: {namespace}\"\n",
    "\n",
    "    if node_selector != \"\":\n",
    "        node_selector = f\"nodeSelector:\\n        {node_selector}\"\n",
    "\n",
    "    train_model_comp_yaml = yaml.safe_load(train_specification)\n",
    "    container_yaml = train_model_comp_yaml[\"implementation\"][\"container\"]\n",
    "    command = container_yaml[\"command\"]\n",
    "    args = container_yaml[\"args\"]\n",
    "\n",
    "    pathParameters = {\n",
    "        \"dataset_directory\": dataset_directory,\n",
    "        \"model_dir\": model_dir\n",
    "    }\n",
    "    \n",
    "    def clone_path(source, target):\n",
    "        try:\n",
    "            logger.info(f\"Cloning source path {source} to {target}...\")\n",
    "            shutil.copytree(source, target)\n",
    "            logger.info(f\"Cloning finished. Target path contents:\")\n",
    "            logger.info(os.listdir(target))\n",
    "        except OSError as e:\n",
    "            if e.errno in (errno.ENOTDIR, errno.EINVAL):\n",
    "                shutil.copy(source, target)\n",
    "            else: raise   \n",
    "    \n",
    "    actual_args = list()\n",
    "    outputs = list()\n",
    "    for idx, arg in enumerate(args):\n",
    "        if type(arg) is dict:\n",
    "            if \"inputValue\" in arg:\n",
    "                # required parameter (value)\n",
    "                key = arg[\"inputValue\"]\n",
    "                if key in train_parameters:\n",
    "                    actual_args.append(train_parameters[key])\n",
    "                else:\n",
    "                    err = f\"Required parameter '{key}' missing in component input!\"\n",
    "                    print(err)\n",
    "                    raise Exception(err)\n",
    "            elif \"if\" in arg:\n",
    "                # optional parameter\n",
    "                key = arg[\"if\"][\"cond\"][\"isPresent\"]\n",
    "                if key in train_parameters:\n",
    "                    actual_args.append(f\"--{key}\")\n",
    "                    actual_args.append(train_parameters[key])\n",
    "            elif \"inputPath\" in arg:\n",
    "                # required InputPath\n",
    "                key = arg[\"inputPath\"]\n",
    "                if key in train_parameters:\n",
    "                    path_key = train_parameters[key]\n",
    "                    if path_key in pathParameters:\n",
    "                        mount = f\"{train_mount}{pathParameters[path_key]}\"\n",
    "                        clone_path(pathParameters[path_key], mount)\n",
    "                        actual_args.append(mount)\n",
    "                    else:\n",
    "                        err = f\"InputPath '{path_key}' unavailable in training component!\"\n",
    "                        print(err)\n",
    "                        raise Exception(err)\n",
    "                else:\n",
    "                    err = f\"Required parameter '{key}' missing in component input!\"\n",
    "                    print(err)\n",
    "                    raise Exception(err)\n",
    "            elif \"outputPath\" in arg:\n",
    "                # required OutputPath\n",
    "                key = arg[\"outputPath\"]\n",
    "                if key in train_parameters:\n",
    "                    path_key = train_parameters[key]\n",
    "                    if path_key in pathParameters:\n",
    "                        mount = f\"{train_mount}{pathParameters[path_key]}\"\n",
    "                        outputs.append((mount, pathParameters[path_key]))\n",
    "                        actual_args.append(mount)\n",
    "                    else:\n",
    "                        err = f\"OutputPath '{path_key}' unavailable in training component!\"\n",
    "                        print(err)\n",
    "                        raise Exception(err)\n",
    "                else:\n",
    "                    err = f\"Required parameter '{key}' missing in component input!\"\n",
    "                    print(err)\n",
    "                    raise Exception(err)\n",
    "        else:\n",
    "            # required parameter (key)\n",
    "            actual_args.append(arg)\n",
    "\n",
    "    train_command = json.dumps(command + actual_args)\n",
    "\n",
    "    logger.info(\"=======================================\")\n",
    "    logger.info(\"Derived configurations\")\n",
    "    logger.info(\"=======================================\")\n",
    "    logger.info(f\"job_name: {job_name}\")\n",
    "    logger.info(f\"namespace: {namespace}\")\n",
    "    logger.info(f\"actual_args: {actual_args}\")\n",
    "    logger.info(f\"train_command: {train_command}\")\n",
    "    logger.info(\"=======================================\")\n",
    "\n",
    "    yaml_objects = list()\n",
    "\n",
    "    if (pvc_name == \"\"):\n",
    "        pvc_name = f\"{job_name}-pvc\"\n",
    "        pvc_spec = f\"\"\"apiVersion: batch/v1\n",
    "apiVersion: v1\n",
    "kind: PersistentVolumeClaim\n",
    "metadata:\n",
    "  name: {pvc_name}\n",
    "  {namespace_spec}\n",
    "spec:\n",
    "  accessModes:\n",
    "  - ReadWriteMany\n",
    "  resources:\n",
    "    requests:\n",
    "      storage: {pvc_size}\n",
    "\"\"\"\n",
    "        yaml_objects.append(yaml.safe_load(pvc_spec))\n",
    "\n",
    "    job_spec = f\"\"\"apiVersion: batch/v1\n",
    "kind: Job\n",
    "metadata:\n",
    "  name: {job_name}\n",
    "  {namespace_spec}\n",
    "spec:\n",
    "  template:\n",
    "    metadata:\n",
    "      annotations:\n",
    "        sidecar.istio.io/inject: \"false\"\n",
    "    spec:\n",
    "      {node_selector}\n",
    "      containers:\n",
    "        - name: training-container\n",
    "          image: {base_image}\n",
    "          command: {train_command}\n",
    "          volumeMounts:\n",
    "            - mountPath: {train_mount}\n",
    "              name: training\n",
    "          restartPolicy: Never\n",
    "          resources:\n",
    "            limits:\n",
    "              cpu: {cpus}\n",
    "              memory: {memory}\n",
    "              nvidia.com/gpu: {gpus}\n",
    "      volumes:\n",
    "        - name: training\n",
    "          persistentVolumeClaim:\n",
    "            claimName: {pvc_name}\n",
    "      restartPolicy: Never\n",
    "\"\"\"\n",
    "    yaml_objects.append(yaml.safe_load(job_spec))\n",
    "\n",
    "    logger.info(f\"Starting Job '{namespace}.{job_name}'\")\n",
    "    utils.create_from_yaml(\n",
    "        client.ApiClient(),\n",
    "        yaml_objects=yaml_objects\n",
    "    )\n",
    "\n",
    "    logger.info(\"Reading job information...\")\n",
    "    job_def = client.BatchV1Api().read_namespaced_job(\n",
    "        name=job_name,\n",
    "        namespace=namespace\n",
    "    )\n",
    "\n",
    "    if logger.isEnabledFor(logging.DEBUG):\n",
    "        logger.debug(f\"Job information: {job_def}\")\n",
    "\n",
    "    logger.info(\"Waiting for Job to succeed...\")\n",
    "    w = watch.Watch()\n",
    "    for event in w.stream(\n",
    "        client.BatchV1Api().list_namespaced_job,\n",
    "        namespace=namespace,\n",
    "        label_selector=f\"job-name={job_name}\",\n",
    "        timeout_seconds=0\n",
    "    ):\n",
    "        object = event['object']\n",
    "\n",
    "        if object.status.succeeded:\n",
    "            w.stop()\n",
    "            logger.info(\"Job finished.\")\n",
    "            break\n",
    "\n",
    "        if not object.status.active and object.status.failed:\n",
    "            w.stop()\n",
    "            logger.error(\"Job Failed!\")\n",
    "            raise Exception(\"Job Failed\")\n",
    "            \n",
    "    logger.info(\"Receiving outputs...\")\n",
    "    for (source, target) in outputs:\n",
    "        clone_path(source, target)\n",
    "    \n",
    "    logger.info(\"Reading logs...\")\n",
    "    pods_list = client.CoreV1Api().list_namespaced_pod(\n",
    "        namespace=namespace,\n",
    "        label_selector=\"controller-uid=\" + job_def.metadata.labels[\"controller-uid\"],\n",
    "        timeout_seconds=10\n",
    "    )\n",
    "    try:\n",
    "        pod_log_response = client.CoreV1Api().read_namespaced_pod_log(\n",
    "            name=pods_list.items[0].metadata.name,\n",
    "            namespace=namespace,\n",
    "            _return_http_data_only=True,\n",
    "            _preload_content=False\n",
    "        )\n",
    "        pod_log = pod_log_response.data.decode(\"utf-8\")\n",
    "    except ApiException as e:\n",
    "        logger.error(f\"Error reading logs: {e}\")\n",
    "\n",
    "    logger.info(\"Deleting Job resources...\")\n",
    "    client.BatchV1Api().delete_namespaced_job(job_name, namespace)\n",
    "\n",
    "    logger.info(\"Preparing outputs...\")\n",
    "    if not os.path.exists(model_dir):\n",
    "        os.makedirs(model_dir)\n",
    "    output = namedtuple(\n",
    "        'TrainModelOutputs',\n",
    "        ['logs']\n",
    "    )\n",
    "\n",
    "    logger.info(\"Finished.\")\n",
    "    return output(pod_log)\n",
    "\n",
    "\n",
    "train_model_job_comp = create_component_from_func(\n",
    "    func=train_model_job,\n",
    "    output_component_file='component.yaml',\n",
    "    base_image=BASE_IMAGE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0960ca6-3f1a-4381-b08d-0b4b98fff112",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

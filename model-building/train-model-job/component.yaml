name: Train model job
description: Trains a model. Once trained, the model is persisted to model_dir.
inputs:
- {name: dataset_directory, type: String, description: Path to the directory with
    training data.}
- {name: train_specification, type: String, description: Training command as generated
    from a Python function using kfp.components.func_to_component_text.}
- {name: train_parameters, type: 'typing.Dict[str, str]', description: 'Dictionary
    mapping formal to actual parameters for the training spacification.                    '}
- {name: train_mount, type: String, description: 'Optional mounting point for training
    data of an existing PVC. Example: "/train".', default: /train, optional: true}
- {name: model_name, type: String, description: 'Optional name of the model. Must
    be unique for the targeted namespace and conform Kubernetes naming conventions.
    Example: my-model.', default: my-model, optional: true}
- {name: base_image, type: String, description: 'Optional base image for model training.
    Example: quay.io/ibm/kubeflow-notebook-image-ppc64le:latest.', default: 'quay.io/ibm/kubeflow-notebook-image-ppc64le:latest',
  optional: true}
- {name: namespace, type: String, description: 'Optional namespace where the Job and
    associated volumes will be created. By default, the same namespace as where the
    pipeline is executed is chosen. Example: "user-example-com".', default: '', optional: true}
- {name: node_selector, type: String, description: 'Optional node selector for worker
    nodes. Example: nvidia.com/gpu.product: "Tesla-V100-SXM2-32GB".', default: '',
  optional: true}
- {name: remote_host, type: String, default: '', optional: true}
- {name: pvc_name, type: String, description: 'Optional name to an existing persistent
    volume claim (pvc). If given, this pvs is mounted into the training job. Example:
    "music-genre-classification-j4ssf-training-pvc".', default: '', optional: true}
- {name: pvc_size, type: String, description: 'Optional size of the storage during
    model training. Storage is mounted into to the Job based on a persitent volume
    claim of the given size. Example: 10Gi.', default: 10Gi, optional: true}
- {name: cpus, type: String, description: 'Optional CPU limit for the job. Example:
    "1000m".', default: '8', optional: true}
- {name: gpus, type: Integer, description: 'Optional number of GPUs for the job. Example:
    2.', default: '0', optional: true}
- {name: memory, type: String, description: 'Optional memory limit for the job. Example:
    "1Gi".', default: 32Gi, optional: true}
outputs:
- {name: model_dir, type: String, description: Target path where the model will be
    stored.}
- {name: logs, type: String}
implementation:
  container:
    image: quay.io/ibm/kubeflow-notebook-image-ppc64le:latest
    command:
    - sh
    - -ec
    - |
      program_path=$(mktemp)
      printf "%s" "$0" > "$program_path"
      python3 -u "$program_path" "$@"
    - "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n   \
      \ os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return file_path\n\
      \ndef train_model_job(\n        dataset_directory,\n        train_specification,\n\
      \        train_parameters,\n        model_dir,\n        train_mount = \"/train\"\
      ,\n        model_name = \"my-model\",\n        base_image = \"quay.io/ibm/kubeflow-notebook-image-ppc64le:latest\"\
      ,\n        namespace = \"\",\n        node_selector = \"\",\n        remote_host\
      \ = \"\",\n        pvc_name = \"\",\n        pvc_size = \"10Gi\",\n        cpus\
      \ = \"8\",\n        gpus = 0,\n        memory = \"32Gi\",\n):\n    '''\n   \
      \ Trains a model. Once trained, the model is persisted to model_dir.\n\n   \
      \         Parameters:\n                    dataset_directory: Path to the directory\
      \ with training data.\n                    train_specification: Training command\
      \ as generated from a Python function using kfp.components.func_to_component_text.\n\
      \                    train_parameters: Dictionary mapping formal to actual parameters\
      \ for the training spacification.                    \n                    model_dir:\
      \ Target path where the model will be stored.\n                    train_mount:\
      \ Optional mounting point for training data of an existing PVC. Example: \"\
      /train\".\n                    model_name: Optional name of the model. Must\
      \ be unique for the targeted namespace and conform Kubernetes naming conventions.\
      \ Example: my-model.\n                    base_image: Optional base image for\
      \ model training. Example: quay.io/ibm/kubeflow-notebook-image-ppc64le:latest.\n\
      \                    namespace: Optional namespace where the Job and associated\
      \ volumes will be created. By default, the same namespace as where the pipeline\
      \ is executed is chosen. Example: \"user-example-com\".\n                  \
      \  node_selector: Optional node selector for worker nodes. Example: nvidia.com/gpu.product:\
      \ \"Tesla-V100-SXM2-32GB\".\n                    pvc_name: Optional name to\
      \ an existing persistent volume claim (pvc). If given, this pvs is mounted into\
      \ the training job. Example: \"music-genre-classification-j4ssf-training-pvc\"\
      .\n                    pvc_size: Optional size of the storage during model training.\
      \ Storage is mounted into to the Job based on a persitent volume claim of the\
      \ given size. Example: 10Gi.\n                    cpus: Optional CPU limit for\
      \ the job. Example: \"1000m\".\n                    gpus: Optional number of\
      \ GPUs for the job. Example: 2.\n                    memory: Optional memory\
      \ limit for the job. Example: \"1Gi\".\n            Returns:\n             \
      \       logs: Result outputs of the Job. Example: \"...Job finished successfully\"\
      .\n    '''\n    from collections import namedtuple\n    from datetime import\
      \ datetime\n    import errno\n    import json\n    from kubernetes import (\n\
      \        client,\n        config,\n        utils,\n        watch\n    )\n  \
      \  from kubernetes.client.rest import ApiException\n    import logging\n   \
      \ import os\n    import shutil\n    import sys\n    import yaml\n\n    logging.basicConfig(\n\
      \        stream=sys.stdout,\n        level=logging.INFO,\n        format='%(levelname)s\
      \ %(asctime)s: %(message)s'\n    )\n    logger = logging.getLogger()\n\n   \
      \ SA_NAMESPACE = \"/var/run/secrets/kubernetes.io/serviceaccount/namespace\"\
      \n\n    logger.info(\"Establishing cluster connection...\")\n    config.load_incluster_config()\n\
      \n    # init configuration variables\n    epoch = datetime.today().strftime('%Y%m%d%H%M%S')\n\
      \    job_name = f\"job-{model_name}-{epoch}\"\n\n    if namespace == \"\":\n\
      \        with open(SA_NAMESPACE) as f:\n            namespace = f.read()\n \
      \   namespace_spec = f\"namespace: {namespace}\"\n\n    if node_selector !=\
      \ \"\":\n        node_selector = f\"nodeSelector:\\n        {node_selector}\"\
      \n\n    train_model_comp_yaml = yaml.safe_load(train_specification)\n    container_yaml\
      \ = train_model_comp_yaml[\"implementation\"][\"container\"]\n    command =\
      \ container_yaml[\"command\"]\n    args = container_yaml[\"args\"]\n\n    pathParameters\
      \ = {\n        \"dataset_directory\": dataset_directory,\n        \"model_dir\"\
      : model_dir\n    }\n\n    def clone_path(source, target):\n        try:\n  \
      \          logger.info(f\"Cloning source path {source} to {target}...\")\n \
      \           shutil.copytree(source, target)\n            logger.info(f\"Cloning\
      \ finished. Target path contents:\")\n            logger.info(os.listdir(target))\n\
      \        except OSError as e:\n            if e.errno in (errno.ENOTDIR, errno.EINVAL):\n\
      \                shutil.copy(source, target)\n            else: raise   \n\n\
      \    actual_args = list()\n    outputs = list()\n    for idx, arg in enumerate(args):\n\
      \        if type(arg) is dict:\n            if \"inputValue\" in arg:\n    \
      \            # required parameter (value)\n                key = arg[\"inputValue\"\
      ]\n                if key in train_parameters:\n                    actual_args.append(train_parameters[key])\n\
      \                else:\n                    err = f\"Required parameter '{key}'\
      \ missing in component input!\"\n                    print(err)\n          \
      \          raise Exception(err)\n            elif \"if\" in arg:\n         \
      \       # optional parameter\n                key = arg[\"if\"][\"cond\"][\"\
      isPresent\"]\n                if key in train_parameters:\n                \
      \    actual_args.append(f\"--{key}\")\n                    actual_args.append(train_parameters[key])\n\
      \            elif \"inputPath\" in arg:\n                # required InputPath\n\
      \                key = arg[\"inputPath\"]\n                if key in train_parameters:\n\
      \                    path_key = train_parameters[key]\n                    if\
      \ path_key in pathParameters:\n                        mount = f\"{train_mount}{pathParameters[path_key]}\"\
      \n                        clone_path(pathParameters[path_key], mount)\n    \
      \                    actual_args.append(mount)\n                    else:\n\
      \                        err = f\"InputPath '{path_key}' unavailable in training\
      \ component!\"\n                        print(err)\n                       \
      \ raise Exception(err)\n                else:\n                    err = f\"\
      Required parameter '{key}' missing in component input!\"\n                 \
      \   print(err)\n                    raise Exception(err)\n            elif \"\
      outputPath\" in arg:\n                # required OutputPath\n              \
      \  key = arg[\"outputPath\"]\n                if key in train_parameters:\n\
      \                    path_key = train_parameters[key]\n                    if\
      \ path_key in pathParameters:\n                        mount = f\"{train_mount}{pathParameters[path_key]}\"\
      \n                        outputs.append((mount, pathParameters[path_key]))\n\
      \                        actual_args.append(mount)\n                    else:\n\
      \                        err = f\"OutputPath '{path_key}' unavailable in training\
      \ component!\"\n                        print(err)\n                       \
      \ raise Exception(err)\n                else:\n                    err = f\"\
      Required parameter '{key}' missing in component input!\"\n                 \
      \   print(err)\n                    raise Exception(err)\n        else:\n  \
      \          # required parameter (key)\n            actual_args.append(arg)\n\
      \n    train_command = json.dumps(command + actual_args)\n\n    logger.info(\"\
      =======================================\")\n    logger.info(\"Derived configurations\"\
      )\n    logger.info(\"=======================================\")\n    logger.info(f\"\
      job_name: {job_name}\")\n    logger.info(f\"namespace: {namespace}\")\n    logger.info(f\"\
      actual_args: {actual_args}\")\n    logger.info(f\"train_command: {train_command}\"\
      )\n    logger.info(\"=======================================\")\n\n    yaml_objects\
      \ = list()\n\n    if (pvc_name == \"\"):\n        pvc_name = f\"{job_name}-pvc\"\
      \n        pvc_spec = f\"\"\"apiVersion: batch/v1\napiVersion: v1\nkind: PersistentVolumeClaim\n\
      metadata:\n  name: {pvc_name}\n  {namespace_spec}\nspec:\n  accessModes:\n \
      \ - ReadWriteMany\n  resources:\n    requests:\n      storage: {pvc_size}\n\"\
      \"\"\n        yaml_objects.append(yaml.safe_load(pvc_spec))\n\n    job_spec\
      \ = f\"\"\"apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: {job_name}\n\
      \  {namespace_spec}\nspec:\n  template:\n    metadata:\n      annotations:\n\
      \        sidecar.istio.io/inject: \"false\"\n    spec:\n      {node_selector}\n\
      \      containers:\n        - name: training-container\n          image: {base_image}\n\
      \          command: {train_command}\n          volumeMounts:\n            -\
      \ mountPath: {train_mount}\n              name: training\n          restartPolicy:\
      \ Never\n          resources:\n            limits:\n              cpu: {cpus}\n\
      \              memory: {memory}\n              nvidia.com/gpu: {gpus}\n    \
      \  volumes:\n        - name: training\n          persistentVolumeClaim:\n  \
      \          claimName: {pvc_name}\n      restartPolicy: Never\n\"\"\"\n    yaml_objects.append(yaml.safe_load(job_spec))\n\
      \n    logger.info(f\"Starting Job '{namespace}.{job_name}'\")\n    utils.create_from_yaml(\n\
      \        client.ApiClient(),\n        yaml_objects=yaml_objects\n    )\n\n \
      \   logger.info(\"Reading job information...\")\n    job_def = client.BatchV1Api().read_namespaced_job(\n\
      \        name=job_name,\n        namespace=namespace\n    )\n\n    if logger.isEnabledFor(logging.DEBUG):\n\
      \        logger.debug(f\"Job information: {job_def}\")\n\n    logger.info(\"\
      Waiting for Job to succeed...\")\n    w = watch.Watch()\n    for event in w.stream(\n\
      \        client.BatchV1Api().list_namespaced_job,\n        namespace=namespace,\n\
      \        label_selector=f\"job-name={job_name}\",\n        timeout_seconds=0\n\
      \    ):\n        object = event['object']\n\n        if object.status.succeeded:\n\
      \            w.stop()\n            logger.info(\"Job finished.\")\n        \
      \    break\n\n        if not object.status.active and object.status.failed:\n\
      \            w.stop()\n            logger.error(\"Job Failed!\")\n         \
      \   raise Exception(\"Job Failed\")\n\n    logger.info(\"Receiving outputs...\"\
      )\n    for (source, target) in outputs:\n        clone_path(source, target)\n\
      \n    logger.info(\"Reading logs...\")\n    pods_list = client.CoreV1Api().list_namespaced_pod(\n\
      \        namespace=namespace,\n        label_selector=\"controller-uid=\" +\
      \ job_def.metadata.labels[\"controller-uid\"],\n        timeout_seconds=10\n\
      \    )\n    try:\n        pod_log_response = client.CoreV1Api().read_namespaced_pod_log(\n\
      \            name=pods_list.items[0].metadata.name,\n            namespace=namespace,\n\
      \            _return_http_data_only=True,\n            _preload_content=False\n\
      \        )\n        pod_log = pod_log_response.data.decode(\"utf-8\")\n    except\
      \ ApiException as e:\n        logger.error(f\"Error reading logs: {e}\")\n\n\
      \    logger.info(\"Deleting Job resources...\")\n    client.BatchV1Api().delete_namespaced_job(job_name,\
      \ namespace)\n\n    logger.info(\"Preparing outputs...\")\n    if not os.path.exists(model_dir):\n\
      \        os.makedirs(model_dir)\n    output = namedtuple(\n        'TrainModelOutputs',\n\
      \        ['logs']\n    )\n\n    logger.info(\"Finished.\")\n    return output(pod_log)\n\
      \ndef _serialize_str(str_value: str) -> str:\n    if not isinstance(str_value,\
      \ str):\n        raise TypeError('Value \"{}\" has type \"{}\" instead of str.'.format(\n\
      \            str(str_value), str(type(str_value))))\n    return str_value\n\n\
      import json\nimport argparse\n_parser = argparse.ArgumentParser(prog='Train\
      \ model job', description='Trains a model. Once trained, the model is persisted\
      \ to model_dir.')\n_parser.add_argument(\"--dataset-directory\", dest=\"dataset_directory\"\
      , type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
      --train-specification\", dest=\"train_specification\", type=str, required=True,\
      \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--train-parameters\", dest=\"\
      train_parameters\", type=json.loads, required=True, default=argparse.SUPPRESS)\n\
      _parser.add_argument(\"--train-mount\", dest=\"train_mount\", type=str, required=False,\
      \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--model-name\", dest=\"\
      model_name\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
      --base-image\", dest=\"base_image\", type=str, required=False, default=argparse.SUPPRESS)\n\
      _parser.add_argument(\"--namespace\", dest=\"namespace\", type=str, required=False,\
      \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--node-selector\", dest=\"\
      node_selector\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
      --remote-host\", dest=\"remote_host\", type=str, required=False, default=argparse.SUPPRESS)\n\
      _parser.add_argument(\"--pvc-name\", dest=\"pvc_name\", type=str, required=False,\
      \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--pvc-size\", dest=\"pvc_size\"\
      , type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
      --cpus\", dest=\"cpus\", type=str, required=False, default=argparse.SUPPRESS)\n\
      _parser.add_argument(\"--gpus\", dest=\"gpus\", type=int, required=False, default=argparse.SUPPRESS)\n\
      _parser.add_argument(\"--memory\", dest=\"memory\", type=str, required=False,\
      \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--model-dir\", dest=\"\
      model_dir\", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\n\
      _parser.add_argument(\"----output-paths\", dest=\"_output_paths\", type=str,\
      \ nargs=1)\n_parsed_args = vars(_parser.parse_args())\n_output_files = _parsed_args.pop(\"\
      _output_paths\", [])\n\n_outputs = train_model_job(**_parsed_args)\n\n_output_serializers\
      \ = [\n    _serialize_str,\n\n]\n\nimport os\nfor idx, output_file in enumerate(_output_files):\n\
      \    try:\n        os.makedirs(os.path.dirname(output_file))\n    except OSError:\n\
      \        pass\n    with open(output_file, 'w') as f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"
    args:
    - --dataset-directory
    - {inputPath: dataset_directory}
    - --train-specification
    - {inputValue: train_specification}
    - --train-parameters
    - {inputValue: train_parameters}
    - if:
        cond: {isPresent: train_mount}
        then:
        - --train-mount
        - {inputValue: train_mount}
    - if:
        cond: {isPresent: model_name}
        then:
        - --model-name
        - {inputValue: model_name}
    - if:
        cond: {isPresent: base_image}
        then:
        - --base-image
        - {inputValue: base_image}
    - if:
        cond: {isPresent: namespace}
        then:
        - --namespace
        - {inputValue: namespace}
    - if:
        cond: {isPresent: node_selector}
        then:
        - --node-selector
        - {inputValue: node_selector}
    - if:
        cond: {isPresent: remote_host}
        then:
        - --remote-host
        - {inputValue: remote_host}
    - if:
        cond: {isPresent: pvc_name}
        then:
        - --pvc-name
        - {inputValue: pvc_name}
    - if:
        cond: {isPresent: pvc_size}
        then:
        - --pvc-size
        - {inputValue: pvc_size}
    - if:
        cond: {isPresent: cpus}
        then:
        - --cpus
        - {inputValue: cpus}
    - if:
        cond: {isPresent: gpus}
        then:
        - --gpus
        - {inputValue: gpus}
    - if:
        cond: {isPresent: memory}
        then:
        - --memory
        - {inputValue: memory}
    - --model-dir
    - {outputPath: model_dir}
    - '----output-paths'
    - {outputPath: logs}

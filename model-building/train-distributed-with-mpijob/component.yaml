name: Train Distributed with MPIJob
inputs:
- {name: Model Name, type: String, default: 'my-model', description: 'Name of the model. Must be unique for the targeted namespace and conform Kubernetes naming conventions. Example: my-model.'}
- {name: Image, type: String, default: 'quay.io/ibm/kubeflow-notebook-image-ppc64le:elyra3.9.0-py3.8-tensorflow-gpu2.8.0', description: 'Base image for model training. Example: quay.io/ibm/kubeflow-notebook-image-ppc64le:elyra3.9.0-py3.8-tensorflow-gpu2.8.0.'}
- {name: Train Command, type: JsonArray, description: 'MPI-enabled training command. Example: "./train.py".'}
- {name: Namespace, type: String, default: 'user-example-com', description: 'The namespace where the MPIJob and associated volumes will be created. Example: user-example-com.'}
- {name: Number of Workers, type: Integer, default: '1', description: 'Number of worker replicas for the MPIJob. Example: 2.'}
- {name: Number of GPUs, type: Integer, default: '0', description: 'Number of GPUs per worker. Example: 2.'}
- {name: Node Selector, type: String, default: '', description: 'Optional node selector for worker nodes. Example: nvidia.com/gpu.product: "Tesla-V100-SXM2-32GB".'}
- {name: PVC Size, type: String, default: '10Gi', description: 'Size of the storage during model training. Storage is mounted into to the MPIJob based on a persitent volume claim of the given size. Example: 10Gi.'}
- {name: Worker CPUs, type: String, default: '8', description: 'CPU limit for worker nodes. Example: "1000m".'}
- {name: Worker Memory, type: String, default: '32Gi', description: 'Memory limit for worker nodes. Example: "1Gi".'}
- {name: Launcher CPUs, type: String, default: '2', description: 'CPU limit for the launcher node. Example: "1000m".'}
- {name: Launcher Memory, type: String, default: '8Gi', description: 'Memory limit for the launcher node. Example: "1Gi".'}
outputs:
- {name: Results, type: String, description: 'Result outputs of the MPIJob. Example: ...MPIJob finished successfully.'}
metadata:
  annotations:
    author: Sebastian Lehrig <Sebastian.Lehrig1@ibm.com>
implementation:
  container:
    image: quay.io/ibm/kubeflow-component-base-image-k8s-client:latest
    command:
    - bash
    - -exc
    - |
      model_name=${0}
      image=${1}
      train_command=${2}
      namespace=${3}
      number_of_workers=${4}
      number_of_gpus=${5}
      node_selector=${6}
      pvc_size=${7}
      worker_cpus=${8}
      worker_memory=${9}
      launcher_cpus=${10}
      launcher_memory=${11}
      output_results=${12}
      
      if [ "${number_of_gpus}" -lt "1" ]
      then
        slots_per_worker=1
      else
        slots_per_worker=$number_of_gpus
      fi
      number_of_processes=$(($number_of_workers*$slots_per_worker))
      epoch=$(date +%s)
      job_name=mpijob-$model_name-$epoch
      mkdir -p "$(dirname "$output_results")"
      if [ -n "${node_selector}" ]
      then
        node_selector="nodeSelector:
                  ${node_selector}"
      fi

      echo "Starting MPIJob application '${job_name}'"

      cat <<EOF | kubectl apply -f -
      apiVersion: kubeflow.org/v1
      kind: MPIJob
      metadata:
        name: ${job_name}
        namespace: ${namespace}
      spec:
        slotsPerWorker: ${slots_per_worker}
        runPolicy:
          cleanPodPolicy: Running
        mpiReplicaSpecs:
          Launcher:
            replicas: 1
            template:
              metadata:
                annotations:
                  sidecar.istio.io/inject: "false"
              spec:
                containers:
                - image: ${image}
                  name: mpi-launcher
                  command:
                  - mpirun
                  - -np
                  - "${number_of_processes}"
                  - --allow-run-as-root
                  - -bind-to
                  - none
                  - -map-by
                  - slot
                  - --prefix
                  - /opt/conda
                  - -mca
                  - pml
                  - ob1
                  - -mca
                  - btl
                  - ^openib
                  - -x
                  - NCCL_DEBUG=INFO
                  args: ${train_command}
                  resources:
                    limits:
                      cpu: ${launcher_cpus}
                      memory: ${launcher_memory}
          Worker:
            replicas: ${number_of_workers}
            template:
              metadata:
                annotations:
                  sidecar.istio.io/inject: "false"
              spec:
                ${node_selector}
                containers:
                - image: ${image}
                  name: mpi-worker
                  volumeMounts:
                    - mountPath: /train
                      name: training
                  resources:
                    limits:
                      cpu: ${worker_cpus}
                      memory: ${worker_memory}
                      nvidia.com/gpu: ${number_of_gpus}
                volumes:
                  - name: training
                    persistentVolumeClaim:
                      claimName: ${job_name}-pvc
      ---
      apiVersion: v1
      kind: PersistentVolumeClaim
      metadata:
        name: ${job_name}-pvc
        labels:
          app: strategy-volume
      spec:
        accessModes:
        - ReadWriteMany
        resources:
          requests:
            storage: 10Gi
      EOF

      echo "Waiting for MPIJob to be created..."
      kubectl wait -n ${namespace} --for=condition=Created --timeout=600s mpijob/${job_name}

      echo "Waiting for MPIJob to run..."
      kubectl wait -n ${namespace} --for=condition=Running --timeout=600s mpijob/${job_name}

      echo "Waiting for MPIJob to succeed..."
      kubectl wait -n ${namespace} --for=condition=Running=false --timeout=-1s mpijob/${job_name}

      echo "MPIJob finished. Exporting results..."
      
      echo "===================================================================" >> "$output_results"
      echo "== Logs of MPIJob pod ${job_name}-launcher" >> "$output_results"
      echo "== (also gathers logs of all ${number_of_workers} workers)" >> "$output_results"
      echo "===================================================================" >> "$output_results"
      kubectl logs -n ${namespace} ${job_name}-launcher >> "$output_results"
      echo "" >> "$output_results"

      echo "Deleting MPIJob resources..."
      kubectl delete MPIJob -n ${namespace} ${job_name}

      echo "Finished."
    - {inputValue: Model Name}
    - {inputValue: Image}
    - {inputValue: Train Command}
    - {inputValue: Namespace}
    - {inputValue: Number of Workers}
    - {inputValue: Number of GPUs}
    - {inputValue: Node Selector}
    - {inputValue: PVC Size}
    - {inputValue: Worker CPUs}
    - {inputValue: Worker Memory}
    - {inputValue: Launcher CPUs}
    - {inputValue: Launcher Memory}
    - {outputPath: Results}
